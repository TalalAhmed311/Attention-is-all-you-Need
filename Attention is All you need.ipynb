{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6036ef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "22645331",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inpu tEmbedding(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,vocab_size,emb_size,input_length):\n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.vocab_size= vocab_size\n",
    "        \n",
    "        self.input_emb = Embedding(\n",
    "                        input_dim = self.vocab_size,\n",
    "                        output_dim = self.emb_size,input_length = input_length\n",
    "                                  )\n",
    "    \n",
    "    \n",
    "    def __call__(self,x):\n",
    "       \n",
    "        return  self.input_emb(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a9de22ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self,batch_size,seq_len,emb_size):\n",
    "        \n",
    "        positions = np.arange(seq_len)[:,np.newaxis]\n",
    "        depth = np.arange(emb_size)[np.newaxis, :]\n",
    "        depth = (2*depth//2)/emb_size\n",
    "\n",
    "        angle_rates = 1 / (10000**depth)\n",
    "\n",
    "        angle_rads  = positions * angle_rates\n",
    "        angle_rads[:,0::2] = np.sin(angle_rads[:,0::2])\n",
    "        angle_rads[:,1::2] = np.sin(angle_rads[:,1::2])\n",
    "\n",
    "\n",
    "        positions = positions * angle_rads\n",
    "           \n",
    "        self.pos = tf.constant(np.broadcast_to(positions,[batch_size,seq_len,emb_size]))\n",
    "       \n",
    "        \n",
    "    \n",
    "    def __call__(self):\n",
    "        return self.pos\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cac148",
   "metadata": {},
   "source": [
    "# Self  Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eea05fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "06caec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,emb_size,batch_size,heads,seq_len,mode='self'):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        \n",
    "        \n",
    "        \"\"\"\"\n",
    "        Parameters:\n",
    "                emb_size (int): Embedding size (e.g 512)\n",
    "                batch_size (int): Batch Size\n",
    "                heads (int): Number of heads (e.g 8)\n",
    "                seq_len (int): Number of words in each sequence\n",
    "                mode (str): self or mask -> attention\n",
    "                \n",
    "        Returns:\n",
    "            Out (Tensor)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.emb_size= emb_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = emb_size//heads\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Queries, Keys and Values Matrices Layers\n",
    "        self.queries = tf.keras.layers.Dense(self.emb_size)\n",
    "        self.keys = tf.keras.layers.Dense(self.emb_size)\n",
    "        self.values = tf.keras.layers.Dense(self.emb_size)\n",
    "        self.mode = mode\n",
    "    \n",
    "    \n",
    "    def self_attenion(self,queries,keys,values):\n",
    "\n",
    "        \n",
    "        out = tf.zeros([self.batch_size,self.heads,self.seq_len,self.head_dim])\n",
    "        out = tf.Variable(out)\n",
    "        # apply attetion mechanism\n",
    "        for b in range(self.batch_size):\n",
    "            for h in range(self.heads):\n",
    "                for i in range(self.seq_len):\n",
    "                    # Calculating the scores by multiplying Query with key\n",
    "                    \n",
    "                    scores = tf.keras.layers.Dot(axes=-1)([queries[b,h,i,:][tf.newaxis,:], keys[b,h,:,:][tf.newaxis,:]])\n",
    "                    \n",
    "                    # Normalize the scores\n",
    "                    scores = scores/np.sqrt(self.head_dim)\n",
    "                 \n",
    "                    # apply Softmax\n",
    "                    z = tf.keras.activations.softmax(scores[tf.newaxis,:],axis=1)\n",
    "                    z = tf.reshape(z,[self.seq_len,1])\n",
    "                    \n",
    "                    # Multiply the score with value vectors\n",
    "                    value_vectors = tf.cast(values[b,h,:,:],dtype=tf.float32) * tf.cast(z,dtype=tf.float32)\n",
    "\n",
    "\n",
    "                    out[b,h,i,:].assign(tf.reduce_sum(value_vectors,axis=0))\n",
    "\n",
    "    \n",
    "    \n",
    "        return tf.reshape(out,(self.batch_size,self.seq_len,self.emb_size))\n",
    "    \n",
    "    \n",
    "    def mask_attention(self,queries,keys,values):\n",
    "        \n",
    "        out = tf.zeros([self.batch_size,self.heads,self.seq_len,self.head_dim])\n",
    "        out = tf.Variable(out)\n",
    "        # apply attetion mechanism\n",
    "        \n",
    "        for b in range(self.batch_size):\n",
    "            for h in range(self.heads):\n",
    "                for i in range(self.seq_len):\n",
    "                    # Calculating the scores by multiplying Query with key\n",
    "                    scores = tf.keras.layers.Dot(axes=-1)([queries[b,h,i,:][tf.newaxis,:], keys[b,h,:,:][tf.newaxis,:]])\n",
    "                   \n",
    "                    if i<self.seq_len-1:\n",
    "                        \n",
    "                        scores = tf.reshape(scores,self.seq_len)\n",
    "                        inf = [-np.inf for _ in range(i+1,scores.shape[-1])]\n",
    "                        indices = [[j] for j in range(i+1, scores.shape[-1])]\n",
    "                        indices = tf.constant(indices,dtype=tf.int32)\n",
    "                        scores = tf.tensor_scatter_nd_update(scores,indices, inf)\n",
    "\n",
    "                    \n",
    "\n",
    "                    \n",
    "                    # Normalize the scores\n",
    "                    scores = scores/np.sqrt(self.head_dim)\n",
    "                   \n",
    "    #               apply Softmax\n",
    "                    z = tf.keras.activations.softmax(tf.constant(scores)[tf.newaxis,:],axis=1)\n",
    "                    z = tf.reshape(z,[self.seq_len,1])\n",
    "                    \n",
    "                    # Multiply the score with value vectors\n",
    "                    \n",
    "                    \n",
    "                     # Multiply the score with value vectors\n",
    "                    value_vectors = tf.cast(values[b,h,:,:],dtype=tf.float32) * tf.cast(z,dtype=tf.float32)\n",
    "                    out[b,h,i,:].assign(tf.reduce_sum(value_vectors,axis=0))\n",
    "                    \n",
    "\n",
    "    \n",
    "\n",
    "        return tf.reshape(out,(self.batch_size,self.seq_len,self.emb_size))\n",
    "    \n",
    "    def __call__(self,x,enc_key=[],enc_value=[]):\n",
    "        \n",
    "        \n",
    "        # As mention in the paper first we multiply each word embedding in our case 512 with 512x512 Matrcis\n",
    "        # We pass our data through the dense layer\n",
    "        \n",
    "        # For Multiheaded Attention\n",
    "        if len(enc_key)==0 and len(enc_value)==0:\n",
    "            queries = self.queries(x)\n",
    "            keys = self.keys(x)\n",
    "            values = self.values(x)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Multi Headed when keys and values come from encoder part\n",
    "            queries = self.queries(x)\n",
    "            keys = self.keys(enc_key)\n",
    "            values = self.values(enc_value)\n",
    "        \n",
    "\n",
    "        queries = tf.reshape(queries,[self.batch_size,self.heads,self.seq_len,self.head_dim])\n",
    "        keys = tf.reshape(keys,[self.batch_size,self.heads,self.seq_len,self.head_dim])\n",
    "        values = tf.reshape(values,[self.batch_size,self.heads,self.seq_len,self.head_dim])\n",
    "        \n",
    "          \n",
    "        if self.mode == 'self':\n",
    "            \n",
    "            # Self Attention\n",
    "            attention = self.self_attenion(queries,keys,values)\n",
    "            \n",
    "        # Apply masked multiheaded attention\n",
    "        if self.mode == 'mask':\n",
    "            attention = self.mask_attention(queries,keys,values)\n",
    "\n",
    "            \n",
    "        # Last matrix \n",
    "        \n",
    "        out = tf.keras.layers.Dense(self.emb_size)(attention)\n",
    "                          \n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a508eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e893ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0fbe288",
   "metadata": {},
   "source": [
    "# Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ae63111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "               batch_size,\n",
    "               seq_len,\n",
    "               emb_size=512,\n",
    "               heads=8,\n",
    "               forward_expansion=4):\n",
    "    \n",
    "        super(Encoder,self).__init__()\n",
    "\n",
    "        self.emb_size = emb_size\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.heads = heads\n",
    "        self.expansion_rate = forward_expansion\n",
    "\n",
    "        self.mha = MultiHeadAttention(\n",
    "                                  self.emb_size,\n",
    "                                  self.batch_size,\n",
    "                                  self.heads,self.seq_len\n",
    "                                      )\n",
    "        self.dense_1 = tf.keras.layers.Dense(int(self.emb_size*self.expansion_rate),activation='relu')\n",
    "        self.dense_2 = tf.keras.layers.Dense(self.emb_size,activation='relu')\n",
    "\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "\n",
    "    def __call__(self,x):\n",
    "\n",
    "        self_attention = self.mha(x)\n",
    "\n",
    "        x = tf.keras.layers.Add()([x,self_attention])\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        dense = self.dense_1(x)\n",
    "        dense = self.dense_2(dense)\n",
    "\n",
    "        x = tf.keras.layers.Add()([dense,x])\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a739efe4",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "620653a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "               batch_size,\n",
    "               seq_len,\n",
    "               emb_size=512,\n",
    "               heads=8,\n",
    "               forward_expansion=4):\n",
    "        \n",
    "        \n",
    "        super(Decoder,self).__init__()\n",
    "\n",
    "        self.emb_size = emb_size\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.heads = heads\n",
    "        self.expansion_rate = forward_expansion\n",
    "        \n",
    "        \n",
    "        #Mask Multihead Attention\n",
    "        self.causal_attention = MultiHeadAttention(self.emb_size,\n",
    "                                                self.batch_size,\n",
    "                                                self.heads,\n",
    "                                                self.seq_len,\n",
    "                                                mode = 'mask')\n",
    "        \n",
    "        # MultiHeaded Attention\n",
    "        self.mha = MultiHeadAttention(self.emb_size,\n",
    "                                      self.batch_size,\n",
    "                                      self.heads,\n",
    "                                      self.seq_len)\n",
    "        \n",
    "        \n",
    "        self.dense_1 = tf.keras.layers.Dense(int(self.emb_size*self.expansion_rate),activation='relu')\n",
    "        self.dense_2 = tf.keras.layers.Dense(self.emb_size,activation='relu')\n",
    "\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "    def __call__(self,x,enc_key,enc_value):\n",
    "        \n",
    "       \n",
    "        # Apply mask Attention\n",
    "        mask_attention = self.causal_attention(x)\n",
    "        \n",
    "        x = tf.keras.layers.Add()([x,mask_attention])\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        # Self Attention\n",
    "        self_attention = self.mha(x,enc_key,enc_value)\n",
    "        \n",
    "        x = tf.keras.layers.Add()([x,self_attention])\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        dense = self.dense_1(x)\n",
    "        dense = self.dense_2(dense)\n",
    "        \n",
    "        x = tf.keras.layers.Add()([x,dense])\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf970b9d",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b24c4400",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 seq_len,\n",
    "                 batch_size,\n",
    "                 emb_size=512,\n",
    "                 heads=8,\n",
    "                 expansion_rate=4,\n",
    "                 num_modules=6\n",
    "                 ):\n",
    "        \n",
    "        \n",
    "        super(Transformer,self).__init__()\n",
    "        \n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.heads = heads\n",
    "        self.expansion_rate = expansion_rate\n",
    "        self.num_modules = num_modules\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.encoder_layers = [Encoder(batch_size,seq_len) for _ in range(num_modules)]\n",
    "        \n",
    "        self.decoder_layers = [Decoder(batch_size,seq_len) for _ in range(num_modules)]\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(vocab_size,activation='softmax')\n",
    "    \n",
    "    def __call__(self,input1,input2):\n",
    "        \n",
    "        \n",
    "        # Encoder part\n",
    "        \n",
    "        # input embeddings\n",
    "        input_embeddings = InputEmbedding(self.vocab_size,self.emb_size,self.seq_len)(input1)\n",
    "        \n",
    "        #positional encoding\n",
    "        \n",
    "        positional_encodings = PositionalEmbedding(self.batch_size,self.seq_len,self.emb_size)()\n",
    "        \n",
    "        enc_out = tf.keras.layers.Add()([input_embeddings,positional_encodings])\n",
    "        \n",
    "#         print(f'Enc shape {enc_out.shape}')\n",
    "        for layer in self.encoder_layers:\n",
    "            enc_out = layer(enc_out)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Decoder Part\n",
    "        \n",
    "         # input embeddings\n",
    "        input_embeddings = InputEmbedding(self.vocab_size,self.emb_size,self.seq_len)(input2)\n",
    "        \n",
    "        #positional encoding\n",
    "        \n",
    "        positional_encodings = PositionalEmbedding(self.batch_size,self.seq_len,self.emb_size)()\n",
    "        \n",
    "        dec_out = tf.keras.layers.Add()([input_embeddings,positional_encodings])\n",
    "        \n",
    "        for layer in self.decoder_layers:\n",
    "            dec_out = layer(dec_out,enc_out,enc_out)\n",
    "        \n",
    "        \n",
    "        # linear Layer\n",
    "        \n",
    "        out = self.linear(dec_out)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return out\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8183cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Embedding\n",
    "\n",
    "vocab_size = 1000\n",
    "batch_size = 10\n",
    "seq_len = 5\n",
    "\n",
    "\n",
    "X = np.array([[0,0,1,0,1],[1,0,1,1,1],[1,0,1,0,1],[0,0,0,0,1],[0,0,1,0,0]\\\n",
    "             ,[0,1,1,0,1],[1,0,1,0,1],[0,1,1,0,1],[1,0,1,0,0],[0,0,1,1,1]])\n",
    "\n",
    "target = np.array([[0,0,1,0,1],[1,0,1,1,1],[1,0,1,0,1],[0,0,0,0,1],[0,0,1,0,0]\\\n",
    "             ,[0,1,1,0,1],[1,0,1,0,1],[0,1,1,0,1],[1,0,1,0,0],[0,0,1,1,1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "26fed145",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = Transformer(vocab_size,seq_len,batch_size)(X,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1ce3319f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 5, 1000])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bb07e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
