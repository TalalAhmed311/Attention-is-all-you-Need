{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u1kro49H5QVn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbedding(nn.Module):\n",
        "\n",
        "  def __init__(self,vocab_size,emb_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.emb_size = emb_size\n",
        "    self.vocab_size= vocab_size\n",
        "    \n",
        "    self.input_emb = nn.Embedding(vocab_size,emb_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.input_emb(x) * np.sqrt(self.emb_size)"
      ],
      "metadata": {
        "id": "cTKVILAG7dyh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "  def __init__(self,batch_size,seq_len,emb_size):\n",
        "\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.seq_len = seq_len\n",
        "    self.emb_size = emb_size\n",
        "\n",
        "\n",
        "    positions = np.arange(seq_len)[:,np.newaxis]\n",
        "    depth = np.arange(emb_size)[np.newaxis, :]\n",
        "    depth = (2*depth//2)/emb_size\n",
        "\n",
        "    angle_rates = 1 / (10000**depth)\n",
        "\n",
        "    angle_rads  = positions * angle_rates\n",
        "    angle_rads[:,0::2] = np.sin(angle_rads[:,0::2])\n",
        "    angle_rads[:,1::2] = np.sin(angle_rads[:,1::2])\n",
        "\n",
        "\n",
        "    self.positions = positions * angle_rads\n",
        "\n",
        "  def forward(self):\n",
        "    return torch.tensor(np.broadcast_to(self.positions,[self.batch_size,self.seq_len,self.emb_size]),dtype=torch.float32)"
      ],
      "metadata": {
        "id": "vTofs4lt--33"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self,emb_size,batch_size,heads,seq_len,decoder=False,mode='self'):\n",
        "        super(MultiHeadAttention,self).__init__()\n",
        "        \n",
        "        \n",
        "        \"\"\"\"\n",
        "        Parameters:\n",
        "                emb_size (int): Embedding size (e.g 512)\n",
        "                batch_size (int): Batch Size\n",
        "                heads (int): Number of heads (e.g 8)\n",
        "                seq_len (int): Number of words in each sequence\n",
        "                decoder (bool): False (if inputs comes from decoder side)\n",
        "                mode (str): self or mask -> attention\n",
        "\n",
        "                \n",
        "        Returns:\n",
        "            Out (Tensor)\n",
        "        \n",
        "        \"\"\"\n",
        "        self.emb_size= emb_size\n",
        "        self.head_dim = emb_size//heads\n",
        "        self.seq_len = seq_len\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        # Queries, Keys and Values Matrices Layers\n",
        "        self.queries = nn.Linear(self.emb_size,self.emb_size)\n",
        "        self.keys = nn.Linear(self.emb_size,self.emb_size)\n",
        "        self.values = nn.Linear(self.emb_size,self.emb_size)\n",
        "        self.out_projection = nn.Linear(self.emb_size,self.emb_size) \n",
        "        \n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        \n",
        "\n",
        "        self.mode = mode\n",
        "        self.decoder = decoder\n",
        "      \n",
        "    \n",
        "    def self_attention(self,queries,keys,values,masked=False):\n",
        "        \"\"\"\n",
        "         queries: (batch_size,seq_len,dim)\n",
        "         keys: (batch_size,seq_len,dim)\n",
        "         values: (batch_size,seq_len,dim)\n",
        "         \n",
        "        \"\"\"\n",
        "        \n",
        "        scores = torch.matmul(queries,keys.transpose(-2,-1))\n",
        "        scores = scores/np.sqrt(self.head_dim)\n",
        "\n",
        "        if masked:\n",
        "            mask = np.tril(np.ones((self.seq_len,self.seq_len)))\n",
        "            mask[mask==0] = -np.inf\n",
        "            scores = scores + torch.tensor(mask,dtype=torch.float32)\n",
        "        \n",
        "        \n",
        "        scores = self.softmax(scores)\n",
        "        atten = torch.matmul(scores,values)\n",
        "        \n",
        "        return atten \n",
        "    \n",
        "     \n",
        "    def forward(self,x,enc_key=[],enc_value=[]):\n",
        "        \n",
        "        \n",
        "        # As mention in the paper first we multiply each word embedding in our case 512 with 512x512 Matrcis\n",
        "        # We pass our data through the dense layer\n",
        "        \n",
        "        # For Multiheaded Attention\n",
        "        if self.decoder==False:\n",
        "            queries = self.queries(x)\n",
        "            keys = self.keys(x)\n",
        "            values = self.values(x)\n",
        "            \n",
        "        else:\n",
        "            \n",
        "            # Multi Headed when keys and values come from encoder part\n",
        "            queries = self.queries(x)\n",
        "            keys = self.keys(enc_key)\n",
        "            values = self.values(enc_value)\n",
        "        \n",
        "\n",
        "\n",
        "        \n",
        "          \n",
        "        if self.mode == 'self':\n",
        "            \n",
        "            # Self Attention\n",
        "            attention = self.self_attention(queries,keys,values)\n",
        "            \n",
        "        # Apply masked multiheaded attention\n",
        "        if self.mode == 'mask':\n",
        "            attention = self.self_attention(queries,keys,values,masked=True)\n",
        "        \n",
        "        \n",
        "            \n",
        "        # Last Projection Matrix \n",
        "        \n",
        "        out = self.out_projection(attention)\n",
        "                          \n",
        "        return out\n"
      ],
      "metadata": {
        "id": "cju0LisCBUww"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder"
      ],
      "metadata": {
        "id": "ET1UoUXfH6is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "               batch_size,\n",
        "               seq_len,\n",
        "               emb_size=512,\n",
        "               heads=8,\n",
        "               forward_expansion=4):\n",
        "    \n",
        "        super(Encoder,self).__init__()\n",
        "\n",
        "        self.emb_size = emb_size\n",
        "        self.seq_len = seq_len\n",
        "        self.batch_size = batch_size\n",
        "        self.heads = heads\n",
        "        self.expansion_rate = forward_expansion\n",
        "\n",
        "        self.mha = MultiHeadAttention(\n",
        "                                  self.emb_size,\n",
        "                                  self.batch_size,\n",
        "                                  self.heads,self.seq_len\n",
        "                                      )\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dense_1 = nn.Linear(self.emb_size,int(self.emb_size*self.expansion_rate))\n",
        "        self.dense_2 =nn.Linear((self.emb_size * self.expansion_rate),self.emb_size)\n",
        "        \n",
        "        self.layer_norm = nn.LayerNorm(self.emb_size)\n",
        "\n",
        "  \n",
        "    def forward(self,x):\n",
        "\n",
        "        self_attention = self.mha(x)\n",
        "\n",
        "        x = x + self_attention\n",
        "\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        dense = self.dense_1(x)\n",
        "        dense = self.relu(dense)\n",
        "        dense = self.dense_2(dense)\n",
        "        dense = self.relu(dense)\n",
        "                          \n",
        "        x = x + dense\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "-Y5uHnlAH1p8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "YtD4jmhKU2UT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    \n",
        "    def __init__(self,\n",
        "               batch_size,\n",
        "               seq_len,\n",
        "               emb_size=512,\n",
        "               heads=8,\n",
        "               forward_expansion=4):\n",
        "        \n",
        "        \n",
        "        super(Decoder,self).__init__()\n",
        "\n",
        "        self.emb_size = emb_size\n",
        "        self.seq_len = seq_len\n",
        "        self.batch_size = batch_size\n",
        "        self.heads = heads\n",
        "        self.expansion_rate = forward_expansion\n",
        "        \n",
        "        \n",
        "        #Mask Multihead Attention\n",
        "        self.causal_attention = MultiHeadAttention(self.emb_size,\n",
        "                                                self.batch_size,\n",
        "                                                self.heads,\n",
        "                                                self.seq_len,\n",
        "                                                mode = 'mask')\n",
        "        \n",
        "        # MultiHeaded Attention\n",
        "        self.mha = MultiHeadAttention(self.emb_size,\n",
        "                                      self.batch_size,\n",
        "                                      self.heads,\n",
        "                                      self.seq_len,decoder=True)\n",
        "        \n",
        "        \n",
        "\n",
        "        self.dense_1 = nn.Linear(self.emb_size,int(self.emb_size*self.expansion_rate))\n",
        "        self.dense_2 = nn.Linear(int(self.emb_size*self.expansion_rate),self.emb_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(self.emb_size)\n",
        "\n",
        "    def forward(self,x,enc_key,enc_value):\n",
        "        \n",
        "       \n",
        "        # Apply mask Attention\n",
        "        mask_attention = self.causal_attention(x)\n",
        "        \n",
        "        x = x + mask_attention\n",
        "\n",
        "        x = self.layer_norm(x)\n",
        "        \n",
        "        # Self Attention\n",
        "        self_attention = self.mha(x,enc_key,enc_value)\n",
        "        \n",
        "        x = x + self_attention\n",
        "        x = self.layer_norm(x)\n",
        "        \n",
        "        \n",
        "        \n",
        "        dense = self.dense_1(x)\n",
        "        dense = self.relu(dense)\n",
        "        dense = self.dense_2(dense)\n",
        "        dense = self.relu(dense)\n",
        "\n",
        "        x = x + dense\n",
        "        x = self.layer_norm(x)\n",
        "        \n",
        "        return x\n"
      ],
      "metadata": {
        "id": "4JR4bwzzH1cU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer\n"
      ],
      "metadata": {
        "id": "fTGuQBbSc8o_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 src_vocab_size,\n",
        "                 trg_vocab_size,\n",
        "                 seq_len,\n",
        "                 batch_size,\n",
        "                 emb_size=512,\n",
        "                 heads=8,\n",
        "                 expansion_rate=4,\n",
        "                 num_modules=6\n",
        "                 ):\n",
        "        \n",
        "        \n",
        "        super(Transformer,self).__init__()\n",
        "        \n",
        "        \n",
        "        self.src_vocab_size = src_vocab_size\n",
        "        self.trg_vocab_size = trg_vocab_size\n",
        "        self.emb_size = emb_size\n",
        "        self.heads = heads\n",
        "        self.expansion_rate = expansion_rate\n",
        "        self.num_modules = num_modules\n",
        "        self.batch_size = batch_size\n",
        "        self.seq_len = seq_len\n",
        "        \n",
        "        self.encoder_layers = [Encoder(batch_size,seq_len) for _ in range(num_modules)]\n",
        "        \n",
        "        self.decoder_layers = [Decoder(batch_size,seq_len) for _ in range(num_modules)]\n",
        "        \n",
        "        self.linear = nn.Linear(self.emb_size,trg_vocab_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "   \n",
        "    \n",
        "    def forward(self,input1,input2):\n",
        "        \n",
        "        \n",
        "        # Encoder part\n",
        "        \n",
        "        # input embeddings\n",
        "        input_embeddings = InputEmbedding(self.src_vocab_size,self.emb_size)(input1)\n",
        "        \n",
        "        #positional encoding\n",
        "        \n",
        "        positional_encodings = PositionalEmbedding(self.batch_size,self.seq_len,self.emb_size)()\n",
        "        \n",
        "        enc_out = input_embeddings + positional_encodings\n",
        "        \n",
        "\n",
        "\n",
        "        for layer in self.encoder_layers:\n",
        "            enc_out = layer(enc_out)\n",
        "            \n",
        "        \n",
        "        \n",
        "        # Decoder Part\n",
        "        \n",
        "         # input embeddings\n",
        "        input_embeddings = InputEmbedding(self.trg_vocab_size,self.emb_size)(input2)\n",
        "        \n",
        "        #positional encoding\n",
        "        \n",
        "        positional_encodings = PositionalEmbedding(self.batch_size,self.seq_len,self.emb_size)()\n",
        "        \n",
        "        dec_out = input_embeddings + positional_encodings\n",
        "        \n",
        "        for layer in self.decoder_layers:\n",
        "            dec_out = layer(dec_out,enc_out,enc_out)\n",
        "        \n",
        "        # linear Layer\n",
        "        \n",
        "        out = self.linear(dec_out)\n",
        "        \n",
        "        # apply softmax\n",
        "        out = self.softmax(out)\n",
        "\n",
        "        \n",
        "        return out\n"
      ],
      "metadata": {
        "id": "pkZibKrDxx8J"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define Model"
      ],
      "metadata": {
        "id": "u3NP6eKK-GE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "seq_len = 5\n",
        "src_vocab_size = 100\n",
        "trg_vocab_size = 150\n",
        "\n",
        "transformer = Transformer(src_vocab_size ,trg_vocab_size,seq_len,batch_size)\n",
        "transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJH6d3Gz-Hcn",
        "outputId": "7ce1c5b1-f97d-4784-878e-04068ff7af24"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (linear): Linear(in_features=512, out_features=150, bias=True)\n",
              "  (softmax): Softmax(dim=-1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_tokens = torch.randint(1, 100, size=(10, 5))\n",
        "trg_tokens = torch.randint(1, 150, size=(10, 5))\n",
        "\n",
        "out = transformer(src_tokens,trg_tokens)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Dp2XxEZECLc",
        "outputId": "d6ab9f9b-8b65-4eea-ecc4-3eaffa57d663"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 5, 150])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}